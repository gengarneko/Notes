# 如何分析、统计算法的执行效率和资源消耗？

![](https://static001.geekbang.org/resource/image/ed/5a/edc6039771a3bdfa2ff132000710e85a.jpg)

我们知道，数据结构和算法本身解决的是 “快” 和 “省” 的问题，即如何让代码运行得更快，如何让代码更省存储空间。所以，执行利率是算法一个非常重要的考量标准。那么如何衡量呢？那就是时间、空间复杂度分析。

只要讲到数据结构和算法，就一定离不开时间、空间复杂度分析。而且，我个人认为，**复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容就基本上掌握了一半。**

## 1 - 为什么需要复杂度分析？

你可能会疑问，我把代码跑一遍，通过统计、监控，就能得到算法的执行时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种方法能比我实实在在跑一遍得到的数据更准确嘛？

首先，可以肯定地说，你这种评估算法执行侠侣的方法是正确的。很多数据结构和算法还给这种方法起了一个名字：**事后统计法**。但是，这种统计方法有很大的局限性：

- **测试结果非常依赖测试环境**：结果受软硬件条件影响，这个不难理解。
- **测试结果受数据规模的影响很大**：同一个排序算法，待排序数据有序度不同，排序的时间差别会很大。
  - 极端情况下，如果数据已经是有序的，那么排序算法不需要做任何操作，执行时间就很短。
  - 如果测试数据规模太小，测试结果可能无法真实地反映算法的性能。小规模排序，插入算法可能比快排快。

> 所以，我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。这就是我们今天要学的时间、空间复杂度分析方法。

## 2 - 大 O 复杂度表示法

算法的执行效率，粗略地讲，就是算法代码执行时间。但是，如何在不运行代码的情况下，用 “肉眼” 得到一段代码的执行时间呢？

这里有段非常简单的代码，求 1,2,3...n 的累加和。现在，我就带你一块来估算一下这段代码的执行时间。

```c
int cal(int n) {
  int sum = 0;
  int i = 1;
  for (; i <= n; ++i) {
    sum += i;
  }
  return sum;
}
```

从 CPU 的角度来看，这段代码的每一行都执行着类似的操作：**读数据 - 运算 - 写数据**。尽管每行代码对应的 CPU 执行的个数、执行时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为 unit_time。在这个假设的基础之上，这段代码的总执行事件是多少呢？

第 2/3 行代码分别需要 1 个 unitTime 的执行时间，第 4、5 行都运行了 n 遍，所以需要
$$
(2n+2)*unitTime
$$
按照这个分析思路，我们再来看看这段代码：

```c
int cal(int n) {
  int sum = 0;
  int i = 1;
  int j = 1;
  for (; j <= n; ++i) {
  	j = 1;
  	for (; j <= n; ++j) {
      sum += i * j;
  	}
  }
}
```

我们依旧假设每个代码块的执行时间是 `unitTime`，那么这段代码的 `T(n)` 是多少呢？

第 2、3、4 行代码，每行都需要 1 个 `unitTime` 的执行时间，第 5、6 行代码循环执行了 `n` 遍，需要
$$
2n*unitTime
$$
这么多的执行时间。第 7、8 行代码循环执行了 `n^2` 遍。所以整体代码总的执行时间就是：
$$
T(n) = (2n2+2n+3)*unitTime
$$
尽管我们不知道 `unitTime` 的具体值，但是通过这两段代码的推导过程我们可以得到一个很重要的规律：

**所有代码的执行时间 `T(n)` 与每行代码的执行次数 `n` 成正比。 **

我们来总结一个公式，注意，大 O 要登场了：
$$
T(n) = O(f(n))
$$
我们来具体解释一下这个公式：

- `T(n)` ：我们已经讲过了，他代表了代码执行时间；
- `n` ：表示数据规模的大小；
- `f(n)` ：表示每行代码执行的次数总和。因为这是一个公式，所以用 f(n) 来表示。
- `O`：表示代码的执行时间 `T(n)` 与 `f(n)` 表达式成正比。

所以，第一个例子和第二个例子中的式子，就是 **大 O 时间复杂度表示法**。大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示 **代码执行时间随数据规模增长的变化趋势**，所以，也叫做 **渐进时间复杂度**（asymptotic time complexity），简称 **时间复杂度**。

当 `n` 很大的时候，你可以把它想象成 10000、100000。而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大 O 表示法表示刚讲的那两段代码的时间复杂度，就可以记为：
$$
T(n) = O(n)
$$

$$
T(n) = O(n^2)
$$

## 3 - 时间复杂度分析

前面讲了大 O 时间复杂度的由来和表示方法。现在我们来看下，如何分析一段代码的时间复杂度？我这儿有三个比较实用的方法可以分享给你。

### 3.1 只关注循环执行次数最多的一段代码

我刚才说，大 O 这种复杂度表示方法只是表示一种变化趋势。我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。所以，**我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了。**这段核心代码执行次数的 `n` 量级，就是整段要分析代码的时间复杂度。

为了方便理解，依然用第一个例子来讲解：

```c
int cal(int n) {
  int sum = 0;
  int i = 1;
  for (; i <= n; ++i) {
    sum += i;
  }
  return sum;
}
```

其中，第 2、3 行代码都是常量级的执行代码，与 `n` 的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第 4、5 行代码，所以这块代码要重点分析。前面我们也讲过，这两行代码被执行了 `n` 次，所以总的时间复杂度就是 `O(n)`。

### 3.2 加法法则：总复杂度等于量级最大的那段代码的复杂度

我这里还有一段代码，你可以试着分析一哈：

```c
int cal(int n) {
  int sum_1 = 0;
  int p = 1;
  for (; p < 100; ++p) {
    sum_1 += p;
  }
  
  int sum_2 = 0;
  int q = 1;
  for (; q < n; ++q) {
    sum_2 += 1;
  }
  
  int sum_3 = 0;
  int i = 1;
  int j = 1;
  for (; i <= n; ++i) {
    j = 1;
    for (; j <= n; ++j) {
      sum_3 += i * j;
    }
  }
  
  return sum_1 + sum_2 + sum_3;
}
```

显而易见，这是计算三个 `sum` 的值，我们可以分别计算它们的时间复杂度，再取一个量级最大的整段代码的复杂度。

第一段的时间复杂度是多少？这段代码循环执行了 100 次，所以是一个常量的执行时间，跟 `n` 的规模无关。

这里我要再强调一次，即便这段代码循环 10000 次、100000 次，只要是一个已知的数，跟 `n` 无关，照样也是常量级的执行时间。当 `n` 无限大的时候，就可以忽略。尽管对代码的执行时间会有很大的影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。

那么第二段和第三段代码的时间复杂度是多少呢？答案是 `O(n)` 和 `O(n^2)` 。

总和这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就为 `O(n^2)`。也就是说：**总的时间复杂度就等于量级最大的那段代码的时间复杂度。**那我们将这个公式抽象一下就变成了：
$$
T(n)=T_1(n)+T_2(n) = max(O(f(n)),O(g(n))) = O(max(f(n), g(n)))
$$

### 3.3 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积

我刚讲了一个复杂度分析中的加法法则，这儿还有一个 **乘法法则**。类比一下，你应该可以猜出来的。也就是：
$$
T(n)=T_1(n)*T_2(n) =  O(f(n)*g(n))
$$
也就是一个嵌套循环，代码如下：

```c
int cal(int n) {
  int ret = 0;
  int i = 1;
  for (; i < n; ++i) {
    ret = ret + f(i);
  }
}

int f(int n) {
  int sum = 0;
  int i = 1;
  for (; i <= n; ++i) {
    sum += i;
  }
  return sum;
}
```

我们单独看 `cal()` 函数，假设 `f()` 只是一个普通的操作，那第 `4 ~ 6` 行的时间复杂度就是 `T1(n) = O(n)`。但是 `f()` 函数本身不是一个简单的操作，它的复杂度是 `T2(n) = O(n)`，所以整个 `cal()` 的时间复杂度就是 `O(n^2)`。



> 以上我们讲了三种复杂度的分析技巧，不过，不需要太过去硬记，在实践中不断去使用它即可。



## 4 - 几种常见时间复杂度实例分析

虽然代码千差万别，但是常见的复杂度量级并不多，我们在这里总结一下，这些复杂度量级几乎涵盖了你今后可以接触的所有代码的复杂度量级，

![](https://static001.geekbang.org/resource/image/37/0a/3723793cc5c810e9d5b06bc95325bf0a.jpg)

我们可以将上面列出的复杂度量级，粗略地分为两类：

- **多项式量级**
- **非多项式量级**：只有 `O(2^n)` 和 `O(n!)`

当数据规模 `n` 越来越大的时候，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。因此，关于 `NP` 时间复杂度我就不展开讲了。我们主要来看几种常见的 **多项式时间复杂度**。

### 4.1 O(1)

首先你要明确一个概念，`O(1)` 只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。比如这行代码，即便有 3 行，它的时间复杂度也是 `O(1)`，没有 `O(3)` 的说法：

```c
int i = 1;
int j = 2;
int k = 5;
```

我们稍微总结一下，只要代码的执行时间不随 `n` 增大而增大，这样的代码时间复杂度我们都记作 `O(1)`。

**一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是 `O(1)`。**

### 4.2 O(logn)、O(nlogn)

对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。我们通过一个例子：

```css
i = 1;
while (i <= n) {
  i = i * 2;
}
```

根据我们之前讲的复杂度分析方法，第三行代码是循环执行次数最多的。所以，我们只要能够计算出这行代码被执行了多少次，就能知道整段代码的时间复杂度。

从代码中可以得出，变量 `i` 的值从 1 开始取，每次循环就乘以 2。当大于 `n`，循环结束。其实就是这个样子：
$$
2^0,2^1,2^2,2^3 ...2^k...2^x = n
$$
所以，我们只要知道 x 值是多少，就知道这行代码执行的次数了。通过 `2^x = n` 求解 `x` ：`x = log2n`

所以，这段代码的时间复杂度就是
$$
O(log_2n)
$$
现在把代码稍微修改一下：

```c
i = 1;
while (i <= n) {
  i = i * 3;
}
```

根据我们刚刚讲的思路，很简单就能看出来，这段代码的时间复杂度是
$$
O(log_3n)
$$
实际上，不管是以 2 为底，还是 3 或者是 10 为底，我们可以把所有对数阶的时间复杂度都记为
$$
O(logn)
$$
这是为什么呢？因为我们知道，对数之间是可以转换的：
$$
log_3n = log_32 * log_2n = C* log_2n
$$
而我们知道 C 是一个常量，在我们的时间复杂度计算中是忽略的，**在采用大 O 标记复杂度的时候，可以忽略系数，即 `O(Cf(n)) = O(f(n))`** 。所以在对数阶时间复杂度的表示方法里，我们忽*略掉对数的底数*，统一为
$$
O(logn)
$$
这里我们讲了 `O(logn)`，那么 `O(nlogn)` 就很好理解了，我们将它循环 `n` 遍，不就是 `O(nlogn)` 嘛？而且，`O(nlogn)` 也是一种常见的时间复杂度，比如，归序排序、快速排序的时间复杂度都是 `O(nlogn)`。

### 4.3 O(m+n)、O(m*n)

我们来看一种和前面都不一样的时间复杂度，代码的复杂度是由两个数据的规模来决定的：

```c
itn cal(int m, int n) {
  int sum = 0;
  int i = 1;
  for (; i < n; ++i) {
    sum_1 += i;
  }
  
  int sum_2 = 0;
  int j = 1;
  for (l j < n; ++j) {
    sum_2 += j;
  }
  
  return sum_1 + sum_2;
}
```

 从代码中可以看得出，`m` 和 `n` 是表示两个数据规模。我们无法事先评估 `m` 和 `n` 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 `O(m+n)`。

针对这种情况，原来的加法法则就不正确了，我们需要将加法法则改为：
$$
T_1(m) + T_2(n) = O(f(m) + g(n))
$$
但是乘法法则继续有效：
$$
T_1(m)*T_2(n) =  O(f(m)*f(n))
$$

## 5 - 空间复杂度

前面我们花了大量精力来讲大 O 表示法和时间复杂度分析，理解了前面的空间复杂度也就很简单去学了。

前面我讲过，时间复杂度的全称是**渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。**类比一下，空间复杂度全称就是**渐进空间复杂度**（asymptotic space complexity），**表示算法的存储空间与数据规模之间的增长关系。**













































