![](https://static001.geekbang.org/resource/image/ed/5a/edc6039771a3bdfa2ff132000710e85a.jpg)

# 如何分析、统计算法的执行效率和资源消耗？

我们知道，数据结构和算法本身解决的是 “快” 和 “省” 的问题，即如何让代码运行得更快，如何让代码更省存储空间。所以，执行利率是算法一个非常重要的考量标准。那么如何衡量呢？那就是时间、空间复杂度分析。

只要讲到数据结构和算法，就一定离不开时间、空间复杂度分析。而且，我个人认为，**复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容就基本上掌握了一半。**

## 1 - 为什么需要复杂度分析？

你可能会疑问，我把代码跑一遍，通过统计、监控，就能得到算法的执行时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种方法能比我实实在在跑一遍得到的数据更准确嘛？

首先，可以肯定地说，你这种评估算法执行侠侣的方法是正确的。很多数据结构和算法还给这种方法起了一个名字：**事后统计法**。但是，这种统计方法有很大的局限性：

- **测试结果非常依赖测试环境**：结果受软硬件条件影响，这个不难理解。
- **测试结果受数据规模的影响很大**：同一个排序算法，待排序数据有序度不同，排序的时间差别会很大。
  - 极端情况下，如果数据已经是有序的，那么排序算法不需要做任何操作，执行时间就很短。
  - 如果测试数据规模太小，测试结果可能无法真实地反映算法的性能。小规模排序，插入算法可能比快排快。

> 所以，我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法。这就是我们今天要学的时间、空间复杂度分析方法。

## 2 - 大 O 复杂度表示法

算法的执行效率，粗略地讲，就是算法代码执行时间。但是，如何在不运行代码的情况下，用 “肉眼” 得到一段代码的执行时间呢？

这里有段非常简单的代码，求 1,2,3...n 的累加和。现在，我就带你一块来估算一下这段代码的执行时间。

```c
int cal(int n) {
  int sum = 0;
  int i = 1;
  for (; i <= n; ++i) {
    sum += i;
  }
  return sum;
}
```

从 CPU 的角度来看，这段代码的每一行都执行着类似的操作：**读数据 - 运算 - 写数据**。尽管每行代码对应的 CPU 执行的个数、执行时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为 unit_time。在这个假设的基础之上，这段代码的总执行事件是多少呢？

第 2/3 行代码分别需要 1 个 unitTime 的执行时间，第 4、5 行都运行了 n 遍，所以需要
$$
(2n+2)*unitTime
$$
按照这个分析思路，我们再来看看这段代码：

```c
int cal(int n) {
  int sum = 0;
  int i = 1;
  int j = 1;
  for (; j <= n; ++i) {
  	j = 1;
  	for (; j <= n; ++j) {
      sum += i * j;
  	}
  }
}
```

我们依旧假设每个代码块的执行时间是 `unitTime`，那么这段代码的 `T(n)` 是多少呢？

第 2、3、4 行代码，每行都需要 1 个 `unitTime` 的执行时间，第 5、6 行代码循环执行了 `n` 遍，需要
$$
2n*unitTime
$$
这么多的执行时间。第 7、8 行代码循环执行了 `n^2` 遍。所以整体代码总的执行时间就是：
$$
T(n) = (2n2+2n+3)*unitTime
$$
尽管我们不知道 `unitTime` 的具体值，但是通过这两段代码的推导过程我们可以得到一个很重要的规律：

**所有代码的执行时间 `T(n)` 与每行代码的执行次数 `n` 成正比。 **

我们来总结一个公式，注意，大 O 要登场了：
$$
T(n) = O(f(n))
$$
我们来具体解释一下这个公式：

- `T(n)` ：我们已经讲过了，他代表了代码执行时间；
- `n` ：表示数据规模的大小；
- `f(n)` ：表示每行代码执行的次数总和。因为这是一个公式，所以用 f(n) 来表示。
- `O`：表示代码的执行时间 `T(n)` 与 `f(n)` 表达式成正比。

所以，第一个例子和第二个例子中的式子，就是 **大 O 时间复杂度表示法**。大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示 **代码执行时间随数据规模增长的变化趋势**，所以，也叫做 **渐进时间复杂度**（asymptotic time complexity），简称 **时间复杂度**。

当 `n` 很大的时候，你可以把它想象成 10000、100000。而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了，如果用大 O 表示法表示刚讲的那两段代码的时间复杂度，就可以记为：
$$
T(n) = O(n)
$$

$$
T(n) = O(n^2)
$$

## 3 - 时间复杂度分析

前面讲了大 O 时间复杂度的由来和表示方法。现在我们来看下，如何分析一段代码的时间复杂度？我这儿有三个比较实用的方法可以分享给你。

### 3.1 只关注循环执行次数最多的一段代码

我刚才说，大 O 这种复杂度表示方法只是表示一种变化趋势。我们通常会忽略掉公式中的常量、低阶、系数，只需要记录一个最大阶的量级就可以了。所以，**我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了。**这段核心代码执行次数的 `n` 量级，就是整段要分析代码的时间复杂度。

为了方便理解，依然用第一个例子来讲解：

```c
int cal(int n) {
  int sum = 0;
  int i = 1;
  for (; i <= n; ++i) {
    sum += i;
  }
  return sum;
}
```

其中，第 2、3 行代码都是常量级的执行代码，与 `n` 的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第 4、5 行代码，所以这块代码要重点分析。前面我们也讲过，这两行代码被执行了 `n` 次，所以总的时间复杂度就是 `O(n)`。

### 3.2 加法法则：总复杂度等于量级最大的那段代码的复杂度

我这里还有一段代码，你可以试着分析一哈：

```c
int cal(int n) {
  int sum_1 = 0;
  int p = 1;
  for (; p < 100; ++p) {
    sum_1 += p;
  }
  
  int sum_2 = 0;
  int q = 1;
  for (; q < n; ++q) {
    sum_2 += 1;
  }
  
  int sum_3 = 0;
  int i = 1;
  int j = 1;
  for (; i <= n; ++i) {
    j = 1;
    for (; j <= n; ++j) {
      sum_3 += i * j;
    }
  }
  
  return sum_1 + sum_2 + sum_3;
}
```

显而易见，这是计算三个 `sum` 的值，我们可以分别计算它们的时间复杂度，再取一个量级最大的整段代码的复杂度。

第一段的时间复杂度是多少？这段代码循环执行了 100 次，所以是一个常量的执行时间，跟 `n` 的规模无关。

这里我要再强调一次，即便这段代码循环 10000 次、100000 次，只要是一个已知的数，跟 `n` 无关，照样也是常量级的执行时间。当 `n` 无限大的时候，就可以忽略。尽管对代码的执行时间会有很大的影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。

那么第二段和第三段代码的时间复杂度是多少呢？答案是 `O(n)` 和 `O(n^2)` 。

总和这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就为 `O(n^2)`。也就是说：**总的时间复杂度就等于量级最大的那段代码的时间复杂度。**那我们将这个公式抽象一下就变成了：
$$
T(n)=T_1(n)+T_2(n) = max(O(f(n)),O(g(n))) = O(max(f(n), g(n)))
$$

### 3.3 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积

我刚讲了一个复杂度分析中的加法法则，这儿还有一个 **乘法法则**。类比一下，你应该可以猜出来的。也就是：
$$
T(n)=T_1(n)*T_2(n) =  O(f(n)*g(n))
$$
也就是一个嵌套循环，代码如下：

```c
int cal(int n) {
  int ret = 0;
  int i = 1;
  for (; i < n; ++i) {
    ret = ret + f(i);
  }
}

int f(int n) {
  int sum = 0;
  int i = 1;
  for (; i <= n; ++i) {
    sum += i;
  }
  return sum;
}
```

我们单独看 `cal()` 函数，假设 `f()` 只是一个普通的操作，那第 `4 ~ 6` 行的时间复杂度就是 `T1(n) = O(n)`。但是 `f()` 函数本身不是一个简单的操作，它的复杂度是 `T2(n) = O(n)`，所以整个 `cal()` 的时间复杂度就是 `O(n^2)`。



> 以上我们讲了三种复杂度的分析技巧，不过，不需要太过去硬记，在实践中不断去使用它即可。



## 4 - 几种常见时间复杂度实例分析

虽然代码千差万别，但是常见的复杂度量级并不多，我们在这里总结一下，这些复杂度量级几乎涵盖了你今后可以接触的所有代码的复杂度量级，

![](https://static001.geekbang.org/resource/image/37/0a/3723793cc5c810e9d5b06bc95325bf0a.jpg)

我们可以将上面列出的复杂度量级，粗略地分为两类：

- **多项式量级**
- **非多项式量级**：只有 `O(2^n)` 和 `O(n!)`

当数据规模 `n` 越来越大的时候，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长。所以，非多项式时间复杂度的算法其实是非常低效的算法。因此，关于 `NP` 时间复杂度我就不展开讲了。我们主要来看几种常见的 **多项式时间复杂度**。

### 4.1 O(1)

首先你要明确一个概念，`O(1)` 只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。比如这行代码，即便有 3 行，它的时间复杂度也是 `O(1)`，没有 `O(3)` 的说法：

```c
int i = 1;
int j = 2;
int k = 5;
```

我们稍微总结一下，只要代码的执行时间不随 `n` 增大而增大，这样的代码时间复杂度我们都记作 `O(1)`。

**一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是 `O(1)`。**

### 4.2 O(logn)、O(nlogn)

对数阶时间复杂度非常常见，同时也是最难分析的一种时间复杂度。我们通过一个例子：

```css
i = 1;
while (i <= n) {
  i = i * 2;
}
```

根据我们之前讲的复杂度分析方法，第三行代码是循环执行次数最多的。所以，我们只要能够计算出这行代码被执行了多少次，就能知道整段代码的时间复杂度。

从代码中可以得出，变量 `i` 的值从 1 开始取，每次循环就乘以 2。当大于 `n`，循环结束。其实就是这个样子：
$$
2^0,2^1,2^2,2^3 ...2^k...2^x = n
$$
所以，我们只要知道 x 值是多少，就知道这行代码执行的次数了。通过 `2^x = n` 求解 `x` ：`x = log2n`

所以，这段代码的时间复杂度就是
$$
O(log_2n)
$$
现在把代码稍微修改一下：

```c
i = 1;
while (i <= n) {
  i = i * 3;
}
```

根据我们刚刚讲的思路，很简单就能看出来，这段代码的时间复杂度是
$$
O(log_3n)
$$
实际上，不管是以 2 为底，还是 3 或者是 10 为底，我们可以把所有对数阶的时间复杂度都记为
$$
O(logn)
$$
这是为什么呢？因为我们知道，对数之间是可以转换的：
$$
log_3n = log_32 * log_2n = C* log_2n
$$
而我们知道 C 是一个常量，在我们的时间复杂度计算中是忽略的，**在采用大 O 标记复杂度的时候，可以忽略系数，即 `O(Cf(n)) = O(f(n))`** 。所以在对数阶时间复杂度的表示方法里，我们忽*略掉对数的底数*，统一为
$$
O(logn)
$$
这里我们讲了 `O(logn)`，那么 `O(nlogn)` 就很好理解了，我们将它循环 `n` 遍，不就是 `O(nlogn)` 嘛？而且，`O(nlogn)` 也是一种常见的时间复杂度，比如，归序排序、快速排序的时间复杂度都是 `O(nlogn)`。

### 4.3 O(m+n)、O(m*n)

我们来看一种和前面都不一样的时间复杂度，代码的复杂度是由两个数据的规模来决定的：

```c
itn cal(int m, int n) {
  int sum = 0;
  int i = 1;
  for (; i < n; ++i) {
    sum_1 += i;
  }
  
  int sum_2 = 0;
  int j = 1;
  for (l j < n; ++j) {
    sum_2 += j;
  }
  
  return sum_1 + sum_2;
}
```

 从代码中可以看得出，`m` 和 `n` 是表示两个数据规模。我们无法事先评估 `m` 和 `n` 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 `O(m+n)`。

针对这种情况，原来的加法法则就不正确了，我们需要将加法法则改为：
$$
T_1(m) + T_2(n) = O(f(m) + g(n))
$$
但是乘法法则继续有效：
$$
T_1(m)*T_2(n) =  O(f(m)*f(n))
$$

## 5 - 空间复杂度

前面我们花了大量精力来讲大 O 表示法和时间复杂度分析，理解了前面的空间复杂度也就很简单去学了。

前面我讲过，时间复杂度的全称是**渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。**类比一下，空间复杂度全称就是**渐进空间复杂度**（asymptotic space complexity），**表示算法的存储空间与数据规模之间的增长关系。**

还一个有点蠢的代码例子：

```c
void print(int n) {
  int i = 0;
  int[] a = new int[n];
  for (i; i < n; ++i) {
    a[i] = i * i;
  }
  
  for (i = n-1; i >= 0; --i) {
    print out a[i];
  }
}
```

跟时间复杂度分析一样，我们可以看到，第 2 行代码中，我们申请了一个空间存储变量 `i`，但是它是常量阶的，根数据规模 `n` 没有关系，所以我们可以忽略。第 3 行申请了一个大小为 `n` 的 `int` 类型数组，除此之外，剩下的代码都没有占用更多的空间，所以整段代码的空间复杂度就是 `O(n)` 。

我们常用的空间复杂度就是 `O(1)`、`O(n)`、`O(n^2)`，像 `O(logn)`、`O(nlogn)` 这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单得多。所以，对于空间复杂度，我们就讲到这。



## 6 - 阶段小结

基础的复杂度知识到此就讲完了，我们来阶段总结一下。

### 一、什么是复杂度分析？

1. 数据结构和算法解决的是 “如何让计算机更短时间、更省时间地解决问题”。
2. 需要从执行时间和占用空间两个维度来评估数据结构和算法的性能。
3. 分别用时间复杂度和空间复杂度来描述性能问题，统称复杂度。
4. 复杂度描述的是算法执行时间（或占用空间）与数据规模的增长关系。

### 二、为什么要进行复杂度分析？

1. 性能测试相比，复杂度分析不依赖环境，高效、节省、容易操作。
2. 掌握复杂度分析，有利于写出更优秀的代码，利于降低开发、维护成本。

### 三、如何进行复杂度分析？

1. 大 O 表示法
   - **来源**：算法的执行时间与每行代码的执行次数成正比，用 `T(n)=O(f(n))` 表示，其中 `T(n)` 表示算法执行总时间，`f(n)` 表示每行代码执行总数，而 `n` 往往表示数据的规模。
   - **特点**：以时间复杂度为例，由于时间复杂度描述的是算法执行时间与数据规模的增长变化趋势，所以常量阶、低阶以及系数实际上对这种增长趋势是不产生决定性影响的，所以在做时间复杂度分析时忽略这些。
2. 复杂度分析法则
   - 单段代码看高频：比如循环
   - 多段代码取最大：比如一段代码中同时存在单循环和多重循环，取多重循环
   - 嵌套代码求乘积：比如递归、多重循环等
   - 多个规模求加法：比如方法有两个参数控制两个循环的次数，那么这时就取二者复杂度相加。

### 四、常用的复杂度级别？

- 多项式阶：随着数据规模的增长，算法的执行时间/空间占比，按照多项式的比例增长。包括：
  -  `O(1)`（常数阶）
  - `O(logn)`（对数阶）
  - `O(n)`（线性街）
  - `O(nlogn)` （线性对数阶）
  - `O(n^2)` （平方阶）
  - `O(n^3)` （立方阶）
- 非多项式阶：随着数据规模的增长，算法的执行时间和空间占用暴增，这类算法性能极差：
  - `O(2^n)` （指数阶）
  - `O(n!)` （阶乘阶）

### 五、如何掌握好复杂度分析方法？

> 复杂度分析关键在于多练，熟能生巧。



复杂度也叫**渐进复杂度**，包括时间复杂度和空间复杂度，用来分析算法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶的算法，执行效率越低。常见的复杂度也就那么几个，从高到低： `O(1)`、`O(n)`、`O(n^2)`、`O(logn)`、`O(nlogn)` 

![](https://static001.geekbang.org/resource/image/49/04/497a3f120b7debee07dc0d03984faf04.jpg)

**复杂度分析不难，关键在于多练**



> 思考：
>
> 有人说，我们能项目之前都会进行性能测试，再做代码的时间复杂度、空间复杂度分析，是不是多此一举呢？而且，每一段代码都分析一下时间、空间复杂度，是不是很浪费时间呢？



> 不认为这是多此一举，渐进式时间、空间复杂度分析为我们提供了一个很好的理论分析方向，并且它是宿主平台无关的，能够让我们对我们的程序或者算法有一个大致的认识，让我们知道，比如在最坏的情况下程序的执行效率如何，这样我们立刻就对不同的算法有了一个“效率”上的感性认识。
>
> 当然，渐进式时间、空间复杂度分析只是一个理论模型，只能提供给粗略的估计分析，我们不能直接断定就觉得 `O(logn)` 的算法一定优于 `O(n)`，针对不同的宿主环境，不同的数据集，不同的数据量大小，在实际应用上可能性能不同，针对不同的实际情况，进而进行一定的性能基准测试是很有必要的。
>
> 综上所述，事件、空间复杂度分析与性能基准测试并不冲突，而是相辅相成的，但是一个低阶的时间复杂度程序有极大的可能性优于一个高阶的时间复杂度程序，所以在实际编程中，时刻关心理论时间、空间复杂度是有助于产出高效率代码的。同时，时间、空间复杂度分析并不会占用特别多的时间。



![](https://static001.geekbang.org/resource/image/9e/ee/9efe0a80fac815101324e1d4de1e49ee.jpg)

# 浅析最好、最坏、平均、均摊时间复杂度

现在来学习四个复杂度分析方面的知识点：

- 最好情况时间复杂度（best case time complexity）
- 最坏情况时间复杂度（worst case time complexity）
- 平均情况时间复杂度（average case time complexity）
- 均摊情况时间复杂度（amortized case time complexity）

## 7 - 最好、最坏情况时间复杂度

先上一段代码：

```c
// n 表示数组 array 的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
    if (array[i] == x) pos = i;
  }
  return pos;
}
```

很明显的功能，在一个数组中，查找变量 `x` 的下标位置，没找到就返回 -1，我们可以很容易的得出时间复杂度是 `O(n)`，其中，`n` 代表数组的长度。

我们在数组中查找一个数据，并不需要每次把整个数组都遍历一遍，因为有可能中途找到就可以提前结束了。优化一下代码：

```c
// n 表示数组 array 的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
    if (array[i] == x) {
      pos = i;
      break;
    }
  }
  return pos;
}
```

这时候我们的时间复杂度还是 `O(n)` 嘛？

这时候我们的时间复杂度可以是 `O(1)`，也可以是 `O(n)`，不同情况下，代码的时间复杂度是不一样的。

为了表示代码在不同情况下的不同时间复杂度，我们需要引入三个概念：

- 最好情况时间复杂度
- 最坏情况时间复杂度
- 平均情况时间复杂度。

> 顾名思义，前两个是在两种极端情况下出现的时间复杂度。

## 8 - 平均时间复杂度

我们知道极端的情况发生概率很低，为了更好地表示平均情况下的复杂度，我们需要引入另一个概念：平均情况时间复杂度，后面我简称为平均时间复杂度。

我们依然看那个例子，要查找的变量 `x` 在数组中的位置，有 `n+1` 种情况：在数组的 `0 ~ n-1` 位置中和不在数组中。我们把每种情况下，查找需要遍历的元素个数加起来，然后再除以 `n+1` ，就可以得到需要遍历元素的个数平均值：

![](https://static001.geekbang.org/resource/image/d8/2f/d889a358b8eccc5bbb90fc16e327a22f.jpg)

然后我们省略掉系数、低阶、常量，得到平均时间复杂度：`O(n)`

这个结论虽然是正确的，但是计算过程还是有问题，是因为这 `n+1` 种情况，它们出现的频率不一样。因为在不在数组里面的概率是为 `1/2`，这样计算的话就应该是这样：

![](https://static001.geekbang.org/resource/image/36/7f/36c0aabdac69032f8a43368f5e90c67f.jpg)

这个值就是概率论中的**加权平均值**，也叫做**期望值**，所以平均时间复杂度的全称应该叫做**加权平均时间复杂度**或者**期望时间复杂度**。

虽然这段代码的加权平均值仍然是 `O(n)`，但是过程区别很大。

大多数情况下，我们并不需要区分最好、最坏、平均时间复杂度，很多时候，我们使用一个复杂度就好了，只有同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。

## 9 - 均摊时间复杂度

来学习一个更加高级的概念，均摊时间复杂度，以及它对应的分析方法，摊还分析（或者平摊分析）。

均摊时间复杂度，听起来和平均时间复杂度有点像。平均时间复杂度只在某些特殊情况下才会使用，而均摊时间复杂度应用场景比它更加特殊、更加有限：

```c
// n 表示数组 array 的长度
int[] array = new int[n];
int count = 0;

void  insert(int val) {
  if (count == array.length) {
    int sum = 0;
    for (int i = 0; i < array.length; ++i) {
      sum += array[i];
    }
    array[0] = sum;
    count = 1;
  }
  
  array[count] = val;
  ++count;
}
```

这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的 count == array.length 时，我们用 for 循环遍历数组求和，并清空数组，将求和之后的sum 值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。

最理想的情况下，数组中有空闲空间，我们只需要将数据插入到数组下标为 count 的位置就可以了，所以最好情况时间复杂度为 `O(1)` 。

最坏的情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为 `O(n)` 。

平均时间复杂度，有 `n+1` 种情况，插入操作的复杂度都是 `O(1)`，额外的时间复杂度是 `O(n)`，进行加权平均的计算方法，我们求得的平均时间复杂度就是：

![](https://static001.geekbang.org/resource/image/6d/ed/6df62366a60336d9de3bc34f488d8bed.jpg)

我们来比对两个例子：`insert()` 和 `find()`，首先，`find()` 函数在极端的例子下，复杂度才为 `O(1)`，但是 `insert()` 在大部分情况下，时间复杂度都为 `O(1)`。其次，对于 `insert()` 函数，`O(1)` 时间复杂度的插入和 `O(n)` 时间复杂度的插入，出现频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 `O(n)` 插入之后，紧跟着 `n-1` 个 `O(1)` 的插入操作，循环往复。

所以，针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样子，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。针对这种特殊场景，我们引入了一个更加简单的分析方法：**摊还分析发**，通过摊还分析得到的时间复杂度我们起了一个名字，叫**均摊时间复杂度**。

那究竟如何使用摊还分析法来分析算法的均摊时间复杂度呢？

我们继续来分析这个例子，每一次 `O(n)` 的插入操作，都会跟着 `n-1` 次 `O(1)` 的插入操作，所以把耗时多的那次操作均摊到接下来的 `n-1` 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 `O(1)`。这就是均摊分析法的大致思路，你理解了吗？

均摊时间复杂度和摊还分析应用场景比较特殊，所以我们并不会经常用到。为了方便你理解、记忆，这里做一下总结。

对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一起分析，看是否能够较高时间复杂度那次操作的耗时，平摊到其它那些时间复杂度比较低的操作上。而且，在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度等于最好情况时间复杂度。

尽管很多算法类书籍都花了较大篇幅来介绍平均时间复杂度和均摊时间复杂度，但是我认为，**均摊时间复杂度就是一种特殊的平均时间复杂度**，我们没必要花太多时间 去区分它们。你最应该掌握的就是它的分析方法，摊还分析。

## 10 - 小结

今天我们引入三个概念：

- 最好情况时间复杂度
- 最坏情况时间复杂度
- 平均情况时间复杂度
- 摊还时间复杂度

之所以引入这几个概念，是因为同一段代码，在不同输入的情况下，复杂度量级有可能是不一样的。



我们最后来分析一下这个函数的复杂度吧：

```c
// 全局变量，大小为 10 的数组 array，长度为 len，下标 i
int array[] new int[10];
int len = 10;
int i = 0;

// 往数组中添加一个元素
void add(int element) {
  if (i >= len) {
    // 数组空间不够了
    // 重新申请一个 2 倍大小的数组空间
    int new_aray[] = new int[len*2];
    // 把原来 array 数组中的数据依次 copy 到新的数组中
    for (int j = 0; j < len; ++j) {
      new_array[j] = array[j];
    }
    // new_array 复制给 array, array 现在大小时 2 倍 len 了
    array = new_array;
    len = 2 * len;
  }
  // 将 element 放到下标 i 的位置，小标 i 加一
  array[i] = element;
  ++i;
}
```

看到 if 判断就分两种情况来讨论：

- `i < len`：for 循环是不走的，时间复杂度都是 `O(1)`
- `i >= len`：for 循环进行数组 copy，时间复杂度有一次 `O(n)`

然后我们通过三种方式来计算时间复杂度：

- 最好情况：`O(1)`
- 最坏情况：`O(n)`
- 平均情况：`O(1)`
- 平均加权：`O(1)`
- 均摊分析：`O(1)`







 